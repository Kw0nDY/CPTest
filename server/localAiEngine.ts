import OpenAI from "openai";

/**
 * ğŸ¯ ë¡œì»¬ AI ì²˜ë¦¬ ì—”ì§„ - ì™¸ë¶€ Flowise API ì˜ì¡´ì„± ì œê±°
 * 
 * ì£¼ìš” ê¸°ëŠ¥:
 * - OpenAI APIë¥¼ í†µí•œ ë¡œì»¬ AI ì²˜ë¦¬ 
 * - Fallback ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì™¸ë¶€ API ì¥ì•  ëŒ€ì‘
 * - ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° ì²˜ë¦¬
 * - ì—ëŸ¬ ë³µêµ¬ ë° ì¬ì‹œë„ ë¡œì§
 */

export interface AIProcessingOptions {
  maxTokens?: number;
  temperature?: number;
  model?: string;
  contextLimit?: number;
  enableFallback?: boolean;
}

export interface AIProcessingResult {
  response: string;
  tokensUsed: number;
  processingTime: number;
  dataSource: 'local' | 'openai' | 'fallback';
  confidence: number;
  metadata?: {
    model: string;
    promptLength: number;
    dataRows: number;
  };
}

export class LocalAIEngine {
  private openai: OpenAI | null = null;
  private fallbackResponses: Map<string, string> = new Map();
  private isInitialized = false;

  constructor() {
    this.initializeFallbackResponses();
  }

  /**
   * AI ì—”ì§„ ì´ˆê¸°í™”
   */
  async initialize(apiKey?: string): Promise<boolean> {
    try {
      // 1. OpenAI API ì„¤ì • ì‹œë„
      const key = apiKey || process.env.OPENAI_API_KEY;
      if (key) {
        this.openai = new OpenAI({ apiKey: key });
        
        // API í‚¤ í…ŒìŠ¤íŠ¸
        await this.testConnection();
        console.log('âœ… OpenAI API ì—°ê²° ì„±ê³µ');
        this.isInitialized = true;
        return true;
      }
      
      console.warn('âš ï¸ OpenAI API í‚¤ê°€ ì—†ìŒ - Fallback ëª¨ë“œë¡œ ë™ì‘');
      this.isInitialized = true;
      return false;
    } catch (error) {
      console.error('âŒ AI ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
      console.log('ğŸ”„ Fallback ëª¨ë“œë¡œ ì „í™˜');
      this.isInitialized = true;
      return false;
    }
  }

  /**
   * OpenAI API ì—°ê²° í…ŒìŠ¤íŠ¸
   */
  private async testConnection(): Promise<void> {
    if (!this.openai) throw new Error('OpenAI not initialized');
    
    await this.openai.chat.completions.create({
      model: 'gpt-5', // the newest OpenAI model is "gpt-5" which was released August 7, 2025. do not change this unless explicitly requested by the user
      messages: [{ role: 'user', content: 'Hello' }],
      max_tokens: 10
    });
  }

  /**
   * ë©”ì¸ AI ì²˜ë¦¬ í•¨ìˆ˜ - ë°ì´í„° ë¶„ì„ ë° ì§ˆë¬¸ ì‘ë‹µ
   * ğŸ¯ AI ëª¨ë¸ë³„ ë°ì´í„° ê²©ë¦¬ ì§€ì›
   */
  async processQuery(
    userMessage: string,
    uploadedData: any[] = [],
    options: AIProcessingOptions = {},
    modelId?: string
  ): Promise<AIProcessingResult> {
    const startTime = Date.now();
    
    if (!this.isInitialized) {
      await this.initialize();
    }

    const {
      maxTokens = 2000,
      temperature = 0.7,
      model = 'gpt-5', // the newest OpenAI model is "gpt-5" which was released August 7, 2025. do not change this unless explicitly requested by the user
      contextLimit = 4000,
      enableFallback = true
    } = options;

    try {
      // ğŸ¯ ëª¨ë¸ë³„ ë°ì´í„° ê²©ë¦¬ ë¡œê¹…
      if (modelId) {
        console.log(`ğŸ”’ AI ëª¨ë¸ ${modelId}ì— ëŒ€í•œ ê²©ë¦¬ëœ ë°ì´í„° ì²˜ë¦¬: ${uploadedData.length}ê°œ ë ˆì½”ë“œ`);
      }
      
      // 1. OpenAI API ì²˜ë¦¬ ì‹œë„
      if (this.openai) {
        console.log('ğŸ¤– OpenAI ë¡œì»¬ ì²˜ë¦¬ ì‹œì‘');
        return await this.processWithOpenAI(userMessage, uploadedData, {
          maxTokens,
          temperature,
          model,
          contextLimit,
          startTime
        }, modelId);
      }

      // 2. Fallback ì²˜ë¦¬
      if (enableFallback) {
        console.log('ğŸ”„ Fallback ëª¨ë“œë¡œ ì²˜ë¦¬');
        return this.processFallback(userMessage, uploadedData, startTime);
      }

      throw new Error('AI ì²˜ë¦¬ ì—”ì§„ì´ ì‚¬ìš© ë¶ˆê°€');

    } catch (error) {
      console.error('âŒ AI ì²˜ë¦¬ ì˜¤ë¥˜:', error);
      
      // 3. ì—ëŸ¬ ì‹œ Fallback
      if (enableFallback) {
        console.log('ğŸ›¡ï¸ ì—ëŸ¬ ë³µêµ¬: Fallback ëª¨ë“œë¡œ ì „í™˜');
        return this.processFallback(userMessage, uploadedData, startTime);
      }
      
      throw error;
    }
  }

  /**
   * OpenAI APIë¥¼ í†µí•œ ì²˜ë¦¬
   * ğŸ¯ AI ëª¨ë¸ë³„ ë°ì´í„° ê²©ë¦¬ ì§€ì›
   */
  private async processWithOpenAI(
    userMessage: string,
    uploadedData: any[],
    config: {
      maxTokens: number;
      temperature: number;
      model: string;
      contextLimit: number;
      startTime: number;
    },
    modelId?: string
  ): Promise<AIProcessingResult> {
    if (!this.openai) throw new Error('OpenAI not initialized');

    // ë°ì´í„° ìš”ì•½ (ì»¨í…ìŠ¤íŠ¸ ì œí•œ ëŒ€ì‘)
    const dataContext = this.summarizeData(uploadedData, config.contextLimit);
    
    // í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ëª¨ë¸ë³„ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€)
    const systemPrompt = this.createSystemPrompt(dataContext, modelId);
    const prompt = this.createUserPrompt(userMessage, dataContext, modelId);
    
    console.log(`ğŸ“Š ë°ì´í„° ì»¨í…ìŠ¤íŠ¸: ${dataContext.rowCount}ê°œ í–‰, ${prompt.length}ì í”„ë¡¬í”„íŠ¸`);

    try {
      const completion = await this.openai.chat.completions.create({
        model: config.model,
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: prompt }
        ],
        max_tokens: config.maxTokens,
        temperature: config.temperature,
        response_format: { type: "json_object" },
      });

      const response = completion.choices[0].message.content || '';
      const tokensUsed = completion.usage?.total_tokens || 0;
      const processingTime = Date.now() - config.startTime;

      console.log(`âœ… OpenAI ì²˜ë¦¬ ì™„ë£Œ: ${tokensUsed} í† í°, ${processingTime}ms`);

      // JSON ì‘ë‹µ íŒŒì‹±
      let parsedResponse;
      try {
        parsedResponse = JSON.parse(response);
        return {
          response: parsedResponse.answer || parsedResponse.response || response,
          tokensUsed,
          processingTime,
          dataSource: 'openai',
          confidence: parsedResponse.confidence || 0.8,
          metadata: {
            model: config.model,
            promptLength: prompt.length,
            dataRows: uploadedData.length
          }
        };
      } catch (parseError) {
        // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return {
          response,
          tokensUsed,
          processingTime,
          dataSource: 'openai',
          confidence: 0.7,
          metadata: {
            model: config.model,
            promptLength: prompt.length,
            dataRows: uploadedData.length
          }
        };
      }

    } catch (apiError: any) {
      console.error('OpenAI API ì˜¤ë¥˜:', apiError);
      
      // API í•œë„ ì´ˆê³¼ë‚˜ ì„œë¹„ìŠ¤ ì˜¤ë¥˜ ì‹œ fallback
      if (apiError.status === 429 || apiError.status >= 500) {
        console.log('ğŸ”„ API í•œë„/ì„œë¹„ìŠ¤ ì˜¤ë¥˜ - Fallbackìœ¼ë¡œ ì „í™˜');
        throw new Error('API_LIMIT_OR_SERVICE_ERROR');
      }
      
      throw apiError;
    }
  }

  /**
   * Fallback ì²˜ë¦¬ (ë¡œì»¬ ê·œì¹™ ê¸°ë°˜)
   */
  private processFallback(
    userMessage: string,
    uploadedData: any[],
    startTime: number
  ): AIProcessingResult {
    const processingTime = Date.now() - startTime;
    
    // í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
    const analysis = this.analyzeKeywords(userMessage, uploadedData);
    
    return {
      response: analysis.response,
      tokensUsed: 0,
      processingTime,
      dataSource: 'fallback',
      confidence: analysis.confidence,
      metadata: {
        model: 'fallback-rules',
        promptLength: userMessage.length,
        dataRows: uploadedData.length
      }
    };
  }

  /**
   * ë°ì´í„° ìš”ì•½ (ë©”ëª¨ë¦¬ ë° ì»¨í…ìŠ¤íŠ¸ ì œí•œ ëŒ€ì‘)
   */
  private summarizeData(data: any[], maxContext: number): {
    summary: string;
    rowCount: number;
    columns: string[];
    sampleData: any[];
  } {
    if (!data || data.length === 0) {
      return {
        summary: 'ì—…ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.',
        rowCount: 0,
        columns: [],
        sampleData: []
      };
    }

    // ì—´ ì¶”ì¶œ
    const firstRow = data[0];
    const columns = Object.keys(firstRow);
    
    // ìƒ˜í”Œ ë°ì´í„° (ë©”ëª¨ë¦¬ ë³´í˜¸)
    const sampleSize = Math.min(10, data.length);
    const sampleData = data.slice(0, sampleSize);
    
    // í†µê³„ ì •ë³´
    const summary = `ë°ì´í„°ì…‹: ${data.length}ê°œ í–‰, ${columns.length}ê°œ ì—´ (${columns.join(', ')})`;
    
    return {
      summary,
      rowCount: data.length,
      columns,
      sampleData
    };
  }

  /**
   * ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
   * ğŸ¯ AI ëª¨ë¸ë³„ ì»¨í…ìŠ¤íŠ¸ ì§€ì›
   */
  private createSystemPrompt(dataContext: any, modelId?: string): string {
    let prompt = `ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì—…ë¡œë“œëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”.

ë°ì´í„° ì •ë³´:
- ${dataContext.summary}
- ì»¬ëŸ¼: ${dataContext.columns.join(', ')}`;

    if (modelId) {
      prompt += `\n\nğŸ”’ ë³´ì•ˆ ì •ì±…: ì´ ë°ì´í„°ëŠ” AI ëª¨ë¸ ${modelId}ì—ë§Œ ì ‘ê·¼ì´ í—ˆìš©ëœ ê²©ë¦¬ëœ ë°ì´í„°ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ëª¨ë¸ì´ë‚˜ ì‹œìŠ¤í…œê³¼ ê³µìœ í•˜ì§€ ë§ˆì„¸ìš”.`;
    }

    prompt += `\n\nJSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{
  "answer": "ë¶„ì„ ê²°ê³¼ ë‹µë³€",
  "confidence": 0.8,
  "key_insights": ["ì£¼ìš” ì¸ì‚¬ì´íŠ¸1", "ì¸ì‚¬ì´íŠ¸2"],
  "data_summary": "ë°ì´í„° ìš”ì•½"
}`;

    return prompt;
  }

  /**
   * ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„±
   * ğŸ¯ AI ëª¨ë¸ë³„ ë°ì´í„° ì¶”ì  ì§€ì›
   */
  private createUserPrompt(userMessage: string, dataContext: any, modelId?: string): string {
    let prompt = `ì§ˆë¬¸: ${userMessage}\n\n`;
    
    if (modelId) {
      prompt += `ğŸ¯ AI ëª¨ë¸: ${modelId}\n`;
    }
    
    if (dataContext.sampleData.length > 0) {
      prompt += `ìƒ˜í”Œ ë°ì´í„°:\n`;
      prompt += JSON.stringify(dataContext.sampleData.slice(0, 3), null, 2);
      prompt += `\n\nì´ ${dataContext.rowCount}ê°œ í–‰ ì¤‘ ì¼ë¶€ì…ë‹ˆë‹¤.`;
    }
    
    return prompt;
  }

  /**
   * í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ (Fallbackìš©)
   */
  private analyzeKeywords(userMessage: string, data: any[]): {
    response: string;
    confidence: number;
  } {
    const message = userMessage.toLowerCase();
    
    // ë°ì´í„° ê¸°ë³¸ ì •ë³´
    const dataInfo = data.length > 0 ? 
      `ë°ì´í„°ì…‹: ${data.length}ê°œ í–‰, ${Object.keys(data[0] || {}).length}ê°œ ì—´` :
      'ì—…ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.';
    
    // í‚¤ì›Œë“œ íŒ¨í„´ ë¶„ì„
    if (message.includes('ë¶„ì„') || message.includes('ìš”ì•½')) {
      return {
        response: `${dataInfo}\n\nê°„ë‹¨í•œ ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ë” ìì„¸í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” AI ê¸°ëŠ¥ì„ í™œì„±í™”í•´ì£¼ì„¸ìš”.`,
        confidence: 0.6
      };
    }
    
    if (message.includes('ê°œìˆ˜') || message.includes('ìˆ˜ëŸ‰')) {
      return {
        response: `ë°ì´í„° ê°œìˆ˜: ${data.length}ê°œ`,
        confidence: 0.8
      };
    }
    
    if (message.includes('ì»¬ëŸ¼') || message.includes('í•„ë“œ')) {
      const columns = data.length > 0 ? Object.keys(data[0]) : [];
      return {
        response: `ì»¬ëŸ¼: ${columns.join(', ')} (ì´ ${columns.length}ê°œ)`,
        confidence: 0.8
      };
    }
    
    // ê¸°ë³¸ ì‘ë‹µ
    return {
      response: `${dataInfo}\n\ní˜„ì¬ AI ê¸°ëŠ¥ì´ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë” ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ AI ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.`,
      confidence: 0.4
    };
  }

  /**
   * Fallback ì‘ë‹µ ì´ˆê¸°í™”
   */
  private initializeFallbackResponses(): void {
    this.fallbackResponses.set('data_analysis', 'ë°ì´í„°ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. AI ê¸°ëŠ¥ì„ í™œì„±í™”í•˜ë©´ ë” ìì„¸í•œ ë¶„ì„ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.');
    this.fallbackResponses.set('data_summary', 'ë°ì´í„° ìš”ì•½ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.');
    this.fallbackResponses.set('error', 'ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
  }

  /**
   * ì—”ì§„ ìƒíƒœ í™•ì¸
   */
  getStatus(): {
    isInitialized: boolean;
    hasOpenAI: boolean;
    canProcess: boolean;
  } {
    return {
      isInitialized: this.isInitialized,
      hasOpenAI: this.openai !== null,
      canProcess: this.isInitialized
    };
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
export const localAI = new LocalAIEngine();